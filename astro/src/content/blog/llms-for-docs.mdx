---
publish_date: 2025-06-05 
title: "How to Make Your Developer Documentation Work with LLMs: Lessons from the Trenches"
description: Engineering leaders increasingly use LLMs to evaluate authentication solutions, but most documentation isn't optimized for AI consumption. This guide shares four proven strategies we've used at FusionAuth to make our developer docs work better with ChatGPT, Claude, and other AI tools.
authors: Dan Moore
image: /img/blogs/zero-trust-idps/zero-trust-and-how-idps-factor-into-it-header-image.png
categories: Education
tags: llms, technical documentation, astro, genai
excerpt_separator: "{/* more */}"
---
import {RemoteCode} from '@fusionauth/astro-components';

Large language models (LLMs) are in use by more and more developers. They use them to write code, but also to discover and leverage tools. In this post, you will learn how to make your docs more accessible to LLMs and to developers using them, based on our real world experience.

{/* more */}

LLMs and LLM enabled tools such as Claude, ChatGPT and Cursor are used by developers in their daily workflow. In 2024, [a survey from GitHub](https://github.blog/news-insights/research/survey-ai-wave-grows/) found over 97% of the 2000 developers surveyed "used AI coding tools at work at some point".

Here at FusionAuth, we have also noticed a trend in the last few months. We started getting inbound sales calls from people who asked ChatGPT about us. We ask people who install our software where they heard about us from and many answer with a specific LLM name or "LLMs/GenAI".

Helping developers be successful by using our documentation to get to what they are trying to accomplish is something we spend a lot of time on here at FusionAuth. In this post, we'll talk about ways we have modified our documentation to make it work better with LLMs, and show you how to do so.

There are two ways LLMs can help your company:

* discovery, where an LLM surfaces your tool and helps users discover it--basically a replacement for Google
* building, where an LLM helps a developer build using your tool--basically a replacement for Stack Overflow

The boundaries between these are blurry, but definitely exist. Some doc improvements help with one more than the other.

Here's a table of the options that will be examined below.

| Option | Pros | Cons | Primary Use |
|--------|------|------|------|
| Build great docs | Helps everyone, including LLMs, developers and search engines     | Large, ongoing time investment | Discovery and building |
| LLM powered chatbot        | Familiar user interface, aggregates across multiple sources of doc, easy buy decision     | Costs money (or time to build), can be inaccurate     | Building |
| Generate a `llms.txt` file        | Helps LLMs discover and ingest content, can be auto-updated, emerging standard    | Fuzzy effectiveness, updating may be tough depending on where content lives, no current support for multiple sections | Discovery, a bit of building |
| Copy to markdown button        | Lets devs use the LLM of their choosing, leverage LLM for use cases you can't imagine or document     | Development time required, may not be used, UX issues | Building |

Now, let's take a deeper look into each of the options above.

## Foundation First: Building Documentation That Works for Both Humans and AI

The first and most important way to help developers and LLMs use your docs is to, well, write them. If they don't exist, they won't help anyone. In addition to writing them, you should:

* structure them well
* include common frequently asked questions
* keep them up to date
* make them fast to download
* generate them (where generation makes sense)
* make them consistent
* offer public long tail content like a forum

If this seems like boring hard work, that is because it is. Docs are their own product. They are a key way for your community to learn your product. They also allow prospective developers to evaluate your product with lower risk than downloading or signing up. They're a signal about product quality too.

The effort of good documentation pays off in multiple ways, because in addition to helping LLMs, it also helps SEO. Google is still a massive source of traffic. And, following those guidelines above, it helps developers who are reading the docs.

The good folks at Kapa.ai, who provide documentation based LLM chatbots for many companies, including FusionAuth, [have a great blog post](https://www.kapa.ai/blog/optimizing-technical-documentation-for-llms) on this topic. There are many good pieces of advice in that post, but this is their top tip: "Embrace Page Structure and Hierarchy". The reason to do so?

> LLMs excel at navigating structured content and rely on context hints to understand the broader picture. A clear hierarchy of headings and subheadings on a page helps LLMs understand the relationships between different sections of your documentation.

The downside of building and maintaining good documentation is it takes a lot of effort to do so. But if you don't get this right, the rest of the tips are not going to help you out much. Having lots of solid, updated, accurate documentation is the foundation for anything else.

### Success Metrics

After building and maintaining docs, make sure they are working for your LLMs for both discovery and building.

For discovery, ask developers how they found you. We've seen a notable uptick in users mentioning finding FusionAuth via LLMs. In fact, since we started tracking it in our [setup wizard](/docs/get-started/download-and-install/setup-wizard#complete-the-setup-wizard) , it has been one of our top three sources. You can also manually look for references or unique strings from your docs that are returned by an LLM of your choice. You can also examine the traffic to your website using your access logs, and see which of your content is being retrieved by the LLM user agents. This is something you'll need access logs to ascertain, since JavaScript tools like Google analytics don't track direct requests to files at a certain location.

For building, you'd need to understand the questions people are asking in the LLM of their choice. The field of analytics around LLM served content is nascent and I couldn't find a solution better here than asking similar questions in a variety of LLMs and ascertaining the accuracy of the responses. Don't forget to ask for links in the prompt to see if the LLM is sending the user to the right location in your docs.

Here's a sample access log line showing ChatGPT retrieving API documentation.

```
52.255.111.85 - - [01/May/2025:00:17:59 +0000] "GET /docs/apis/two-factor HTTP/2.0" 200 661 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko); compatible; ChatGPT-User/1.0; +https://openai.com/bot"
```

While folks talk about LLM scrapers being a major source of traffic, I looked at our logs from Jan 1 to Jun 5 and didn't see it. Here are the percentages of scrapers that identified themselves and made any request resulting in a `200` status code during that timeframe.

| Scraper                     | Percent Of Traffic |
|-----------------------------|--------------------|
| OpenAI                      | 0.99 | 
| Anthropic                   | 0.55 |
| Perplexity                  | 0.18 |
| GoogleBot (used for Gemini) | 0.54 | 
| BingBog (used for Copilot)  | 0.30 | 

All in all, LLM based traffic for these well known options was, at most, 2.6% of our total traffic over this timeframe. 

This seemed weird, so I looked at all the user agents making successful requests. There were a significant number of requests from other types of bots, but not other high-profile LLMs.

## Deploy an AI-Powered Documentation Assistant (Yes, a Chatbot)

Offering up an LLM based chatbot lets you leverage the docs with a convenient interface. It can be interactive on the docs site and also integrate with other support channels such as Slack, Discord or ticketing systems.

While you can build your own, unless you are an AI focused company wanting to dogfood your solution, you're probably better off buying this functionality, since that is an easy way to trade money for time.

FusionAuth [installed Kapa.ai's chatbot in 2023](/blog/llm-for-fusionauth-documentation) and have been using it heavily ever since. There are alternatives out there but I haven't seen any as focused on technical documentation as Kapa.

We don't just have our techdocs ingested in Kapa, we also ingest our [OpenAPI spec](https://github.com/FusionAuth/fusionauth-openapi), our [YouTube channel](https://www.youtube.com/c/fusionauth), our [forum posts](/community/forum/) and a number of other sources. Kapa has answered tens of thousands of queries, both for external and internal users. It's great for discovery of documentation or other resources.

Make sure whatever chatbot you use can say "I don't know" and includes links to sources. We've had few reported hallucinations, but they do happen. Having the source means users can verify the answers.

### Implementation Details

Find an LLM chatbot vendor and follow their installation instructions.

### Success Metrics

The measure of success here is twofold:

* are people using this tool
* are they finding the answers they need

Both of these metrics should be delivered by any chatbot you use or. If you do end up building a solution (don't do it!) make sure you plan to build this reporting.

## Implement llms.txt: The Emerging Standard for AI Discoverability

Another way you can help developers using LLMs succeed with your docs is to build an `llms.txt` file.

[`llms.txt` is a relatively new standard](https://llms-txt.org/) analogous to the `sitemap.xml` or `robots.txt` files, but for LLMs rather than search engine crawlers. It has a different format to provide maximum content in minimum number of tokens. Markdown is used rather than HTML.

There are actually two files:

* `llms.txt` is a list of URLs with useful content, organized by section. Here's our [docs `llms.txt` file](/docs/llms.txt). 
* `llms-full.txt` is a similar list but includes the full text rather than just the URL of the pages. Here is our [docs `llms-full.txt` file](/docs/llms-full.txt). 

It's unclear if you can have more than one set of `llms.txt` files for different sections of your site; the standard is still evolving. [Here's an open issue to discuss this](https://github.com/AnswerDotAI/llms-txt/issues/59).

These files are typically located at the root of your domain: `https://example.com/llms.txt` for example. This is so LLMs have one place to look.

If you put these files somewhere else or have more than one of them, add a link from your HTML pages to the files. Crawlers for LLMs hunt around, but not too much. We shipped an `llms.txt` file for our docs in December, but it wasn't at the root location above. This file was downloaded by the OpenAI chatbot nearly 60 times as much after we linked to it from our HTML docs pages as before we did.

In addition, when you make these files available to developers browsing your site, they can retrieve it and upload it to an LLM of their choosing. This allows them to interrogate the docs or learn more about the doc structure using their tool of choice.

Ensure these files are automatically updated in the same way your `sitemap.xml` file is. You want any new or updated documentation to be included with minimal effort.

Here's a table showing the relative percentages of downloads of either our `llms.txt` file or our `llms-full.txt` file, Jan 1 to Jun 5. There may be other LLMs downloading this file as well.

| Scraper                     | Percent Of LLMs Downloading |
|-----------------------------|--------------------|
| OpenAI                      | 71.59 | 
| GoogleBot (used for Gemini) | 17.20 | 
| Perplexity                  | 8.22 |
| BingBog (used for Copilot)  | 2.80 | 
| Anthropic                   | 0.19 |

### Implementation Notes

We build these files for our docs section. Here's the code for the `llms.txt` file:

<RemoteCode url="https://raw.githubusercontent.com/FusionAuth/fusionauth-site/refs/heads/main/astro/src/pages/docs/llms.txt.ts" lang="javascript" />

You could make this more sophisticated by giving documents in the collection rankings, but we haven't done that yet. For other sections of our website, we handcrafted this file.

For our documentation `llms-full.txt`, the code looks similar: 

<RemoteCode url="https://raw.githubusercontent.com/FusionAuth/fusionauth-site/refs/heads/main/astro/src/pages/docs/llms-full.txt.ts" lang="javascript" />

This pulls in the entire body (`${doc.body}`) rather than just the URL, but other than that is the same.

We use a lot of includes in our docs. I did some work to inline the include file markdown, but that ended up making the `llms-full.txt` file too big for the popular LLMs' context windows.

### Success Metrics

Are these files downloaded? How often? By what user agents? Is it being primarily downloaded by LLMs or users?

The answers to these questions lie in your access logs, since JavaScript tools like Google Analytics don't track direct requests to files.

## Enable Developer Self-Service with a Copy-to-Markdown Button

Adding a "copy to markdown" button like below helps developers leverage LLMs in a similar way to providing the `llms-full.txt` file mentioned above. Folks can copy the text and paste it into their favorite LLM, while using fewer tokens than the `llms-full.txt` file.

We've installed this on every page on our docs site; an example is below:

![A copy to markdown button on a FusionAuth documentation page.](/img/blogs/llms-for-docs/markdown-button.png)

By offering this button, you let developers transform your documentation for their purpose. Here are some examples of prompts a developer might do for a technical doc like an API:

* summarize all the error statuses of this API
* explain the key concepts here to me "like I'm 5" or "like I'm an expert software architect" or "like I'm a front end developer"
* how does this API compare with a similar API from \[competitor]
* show me how to use this API in Rust or C++ (or other possibly unsupported languages)
* provide me 10 sets of fake API requests for this API to use in my test harness
* translate this page to Spanish or Swedish

Basically, you don't know exactly what is important to the developer at that moment in time, but they do. And they can ask the LLM to provide it.

Of course someone could copy the HTML and then do the transformation, but that uses more tokens and is more cumbersome.

### Implementation Notes

We built JavaScript to perform the copy operation:

<RemoteCode url="https://raw.githubusercontent.com/FusionAuth/fusionauth-site/refs/heads/main/astro/src/tools/docs/copyMarkdownToClipboard.js" lang="javascript" />

We also used an astro `post:build` plugin to generate the markdown file at a sibling location to the HTML, so the JavaScript above could retrieve it.

<RemoteCode url="https://raw.githubusercontent.com/FusionAuth/fusionauth-site/refs/heads/main/astro/src/plugins/markdown-extract.js" lang="javascript" />

This astro plugin code:

* finds all the document pages in a collection
* inlines all the content in `src/diagrams` and `src/content` (we aren't as worried about overflowing the context window with this single page of markdown)
* removes all the frontmatter
* writes a peer markdown file in a specific location

There are still component tags in the output markdown because components provide functionality and HTML. There are alternative solutions that take HTML and turn it into pure markdown we may use in the future.

### Success Metrics

The success metric for this is how many times the copy to markdown button is pressed.

Since this is a user driven event, tracking it via a tool like Google Analytics is great.

Unfortunately, you can't know what transformation is done on your documentation without asking your users.

## Where Is MCP In This?

The [model context protocol](https://modelcontextprotocol.io/introduction), or MCP, allows you to let an LLM directly interface with your developer focused product.

We've explored but not implemented an MCP for FusionAuth. MCP is great for learning a tool, building prototypes, or debugging. But I'm not sure folks will be using it to replace [configuration management](/docs/operate/deploy/configuration-management) in production systems.

MCP is not covered here because it is doc-adjacent but not part of documentation. If you have an opinion about the usefulness of a FusionAuth MCP server, well, there's [an open bug for sharing your feedback](https://github.com/FusionAuth/fusionauth-issues/issues/3046). We'd love to hear from you.

## Summing It Up

LLMs are here.

Developers are using them to interact with your documentation.

Use one, two, or all of the options presented above to make it easier for them to do so.
